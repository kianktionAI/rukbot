import os
import math
import heapq
import fitz  # PyMuPDF
from dotenv import load_dotenv
from openai import OpenAI
from datetime import datetime
import gspread
from google.oauth2.service_account import Credentials
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, HTMLResponse
from fastapi.templating import Jinja2Templates
from fastapi.staticfiles import StaticFiles
from fastapi.middleware.cors import CORSMiddleware

from drive_utils import load_google_folder_files

# =====================================================
# 1Ô∏è‚É£ ENVIRONMENT SETUP
# =====================================================
load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_PROJECT_ID = os.getenv("OPENAI_PROJECT_ID")
GOOGLE_DRIVE_FOLDER_ID = os.getenv(
    "GOOGLE_DRIVE_FOLDER_ID", "12ZRNwCmVa3d2X5-rBQrbzq7f9aIDesiV"
)

print("üöÄ Starting RukBot server...")
print(f"üß© Using Drive folder ID: {GOOGLE_DRIVE_FOLDER_ID}")

# Retrieval tuning (safe defaults)
CHUNK_SIZE = 800
CHUNK_OVERLAP = 120
TOP_K = 6
MIN_SIMILARITY = 0.58  # lower = less rigid, higher = more cautious

# =====================================================
# 2Ô∏è‚É£ FASTAPI APP CONFIGURATION
# =====================================================
app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# =====================================================
# 3Ô∏è‚É£ OPENAI CLIENT SETUP
# =====================================================
client = OpenAI(api_key=OPENAI_API_KEY, project=OPENAI_PROJECT_ID)

# =====================================================
# 4Ô∏è‚É£ KNOWLEDGE BASE: LOAD + CHUNK + EMBED
# =====================================================
def chunk_text(text: str, size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP):
    chunks, start = [], 0
    n = len(text)
    step = max(1, size - overlap)
    while start < n:
        end = min(n, start + size)
        chunks.append(text[start:end])
        start += step
    return chunks


def cosine_similarity(a, b):
    # a and b are Python lists of floats
    dot = 0.0
    na = 0.0
    nb = 0.0
    for xa, xb in zip(a, b):
        dot += xa * xb
        na += xa * xa
        nb += xb * xb
    if na == 0.0 or nb == 0.0:
        return 0.0
    return dot / (math.sqrt(na) * math.sqrt(nb))


def get_embedding_vec(text: str):
    try:
        resp = client.embeddings.create(model="text-embedding-3-small", input=text)
        # returns list[float]
        return resp.data[0].embedding
    except Exception as e:
        print(f"‚ö†Ô∏è Embedding error: {e}")
        return []


def build_knowledge_index(file_dict):
    """
    Returns:
      {
        "chunks": [{"filename":..., "chunk_index":..., "text":..., "embedding":[...]}],
      }
    """
    index = {"chunks": []}
    total_chunks = 0

    for filename, content in file_dict.items():
        if not content or not content.strip():
            continue
        parts = chunk_text(content)
        for i, part in enumerate(parts):
            emb = get_embedding_vec(part)
            index["chunks"].append(
                {
                    "filename": filename,
                    "chunk_index": i,
                    "text": part,
                    "embedding": emb,
                }
            )
            total_chunks += 1

    print(f"‚úÖ Knowledge index built: {len(file_dict)} files ‚Üí {total_chunks} chunks.")
    return index


print("üìÇ Loading raw knowledge from Google Drive...")
try:
    knowledge_files = load_google_folder_files(GOOGLE_DRIVE_FOLDER_ID)
    knowledge_index = build_knowledge_index(knowledge_files)
except Exception as e:
    print(f"‚ùå Error preparing knowledge base: {e}")
    knowledge_files = {}
    knowledge_index = {"chunks": []}

response_count = 0  # tracks first vs follow-up (if you want to use it later)

# =====================================================
# 5Ô∏è‚É£ GOOGLE SHEET LOGGING
# =====================================================
def log_to_google_sheet(question, response):
    try:
        creds_path = (
            "/etc/secrets/service_account.json"
            if os.getenv("RENDER")
            else "service_account_rukbot.json"
        )
        creds = Credentials.from_service_account_file(
            creds_path, scopes=["https://www.googleapis.com/auth/spreadsheets"]
        )
        sheet = gspread.authorize(creds).open("RukBot Logs")
        worksheet = sheet.worksheet("Sheet1")
        worksheet.append_row(
            [datetime.now().strftime("%Y-%m-%d %H:%M:%S"), question, response]
        )
    except Exception as e:
        print(f"‚ö†Ô∏è Logging to Google Sheet failed: {e}")

# =====================================================
# 6Ô∏è‚É£ (Optional) PDF extraction utility
# =====================================================
def extract_text_from_pdf(filename):
    text = ""
    try:
        with fitz.open(filename) as doc:
            for page in doc:
                text += page.get_text()
    except Exception as e:
        print(f"Error reading {filename}: {e}")
    return text

# =====================================================
# 7Ô∏è‚É£ RETRIEVAL
# =====================================================
PRODUCT_HINTS = {
    "rukvest": ["RUKVEST", "RUKVEST_Product_Info.pdf"],
    "vest": ["RUKVEST", "RUKVEST_Product_Info.pdf"],
    "ruksak": ["RUKSAK", "RUKSAK_Product_Info.pdf"],
    "bag": ["RUKSAK", "RUKSAK_Product_Info.pdf"],
    "rucksack": ["RUKSAK", "RUKSAK_Product_Info.pdf"],
    "rukbrik": ["RUKBRIK", "RUKBRIK_Product_Info.pdf"],
    "brick": ["RUKBRIK", "RUKBRIK_Product_Info.pdf"],
}


def candidate_chunks(user_text: str):
    """
    Light filter: if the query contains product words,
    prefer chunks whose filename matches those hints.
    Falls back to all chunks if no candidates found.
    """
    msg = user_text.lower()
    preferred_files = set()
    for key, hints in PRODUCT_HINTS.items():
        if key in msg:
            for h in hints:
                preferred_files.add(h)

    if not preferred_files:
        return knowledge_index["chunks"]

    filtered = [
        c
        for c in knowledge_index["chunks"]
        if any(h.lower() in c["filename"].lower() or h.lower() in c["text"].lower() for h in preferred_files)
    ]
    return filtered if filtered else knowledge_index["chunks"]


def retrieve(user_text: str, top_k: int = TOP_K):
    """
    Embed user text, score against candidate chunks, return top_k with scores.
    """
    q_emb = get_embedding_vec(user_text)
    if not q_emb:
        return []

    cands = candidate_chunks(user_text)
    scored = []
    for c in cands:
        emb = c.get("embedding", [])
        if not emb:
            continue
        sim = cosine_similarity(q_emb, emb)
        scored.append((sim, c))

    top = heapq.nlargest(top_k, scored, key=lambda x: x[0])
    return top  # list of (score, chunk)

# =====================================================
# 8Ô∏è‚É£ PROMPT GENERATION
# =====================================================
def build_prompt(user_message, context_snippets):
    joined = "\n\n---\n".join(
        [f"[{c['filename']} #{c['chunk_index']}] {c['text']}" for c in context_snippets]
    )
    return f"""
You are RukBot ‚Äî the casually brilliant AI trained on the RUKVEST and RUKSAK brand.

üó£Ô∏è Tone & Style:
- Friendly, like a helpful gym buddy
- Short, sharp, and mobile-friendly
- Add emojis sparingly for emphasis
- Speak human ‚Äî no corporate jargon, no sales pitch

‚ùå Avoid:
- Greetings (‚ÄúHey there‚Äù, ‚ÄúHi‚Äù, ‚ÄúHello‚Äù)
- Mentioning sources, documents, or PDFs
- Overly long or robotic answers

üéØ Mission:
Give clear, confident answers to help customers quickly.
If uncertain, reply exactly with:
‚ÄúI‚Äôm not 100% on that one ‚Äî best to check with our team at team@ruksak.com ‚Äî they‚Äôve got your back!‚Äù

üß† Customer asked:
"{user_message}"

üìö Relevant Knowledge (internal notes):
{joined[:12000]}
"""

# =====================================================
# 9Ô∏è‚É£ RESPONSE GENERATION
# =====================================================
def get_full_response(user_input):
    text = user_input.lower()

    # Disallow mixed product combos (RUKVEST + RUKBRIK, etc.)
    invalid_pairs = [
        ("rukvest", "rukbrik"),
        ("rukbrik", "rukvest"),
        ("rukvest", "rukblock"),
        ("rukblock", "rukvest"),
    ]
    for a, b in invalid_pairs:
        if a in text and b in text:
            print("‚ö†Ô∏è Cross-product combo detected ‚Äî triggering fallback.")
            return (
                "That combo doesn‚Äôt sound right ‚Äî best to check with our team at üì© team@ruksak.com ‚Äî they‚Äôve got your back!"
            )

    # Retrieve top chunks
    top = retrieve(user_input, TOP_K)
    if not top:
        print("‚ö†Ô∏è Retrieval returned no results ‚Äî fallback.")
        return (
            "I‚Äôm not 100% on that one ‚Äî best to check with our team at üì© team@ruksak.com ‚Äî they‚Äôve got your back!"
        )

    top_scores = [s for s, _ in top]
    best_score = top_scores[0] if top_scores else 0.0
    print(f"üß≠ Best semantic match: {best_score:.3f}")

    # If even the best match is weak, be cautious
    if best_score < MIN_SIMILARITY:
        print("‚ö†Ô∏è Low semantic match ‚Äî cautious fallback.")
        return (
            "I‚Äôm not 100% on that one ‚Äî best to check with our team at üì© team@ruksak.com ‚Äî they‚Äôve got your back!"
        )

    # Build prompt with the chunks only (no file/source mention in output)
    context = [c for _, c in top]
    prompt = build_prompt(user_input, context)

    strict_system_message = (
        "You are RukBot ‚Äî the casually brilliant AI for RUKVEST & RUKSAK. "
        "Only answer using the information provided in the prompt's knowledge section. "
        "If you cannot confidently find the answer, you MUST reply exactly with:\n"
        "‚ÄúI‚Äôm not 100% on that one ‚Äî best to check with our team at team@ruksak.com ‚Äî they‚Äôve got your back!‚Äù\n\n"
        "Tone: concise, confident, kind. "
        "Never open with greetings. Never mention 'documents' or 'sources'. "
        "Use emojis only when it improves clarity."
    )

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": strict_system_message},
                {"role": "user", "content": prompt},
            ],
            temperature=0.4,
        )
        answer = response.choices[0].message.content.strip()

        # Safety: ensure fallback phrasing if the model hedges
        unsure_triggers = ["not sure", "unsure", "uncertain", "don‚Äôt know", "can't tell"]
        if any(term in answer.lower() for term in unsure_triggers):
            print("‚ö†Ô∏è Detected uncertainty phrase ‚Äî fallback triggered.")
            return (
                "I‚Äôm not 100% on that one ‚Äî best to check with our team at üì© team@ruksak.com ‚Äî they‚Äôve got your back!"
            )

        # Add contact line if needed when it's an advisory answer
        if "team@ruksak.com" not in answer.lower() and any(
            k in user_input.lower() for k in ["contact", "help", "support", "email"]
        ):
            answer += " üì© If you want to double-check, our team‚Äôs always happy to help at team@ruksak.com ‚Äî they‚Äôve got your back!"

        return answer

    except Exception as e:
        print(f"‚ö†Ô∏è OpenAI request failed: {e}")
        return (
            "Something went a bit sideways there ‚Äî best to check with our team at üì© team@ruksak.com ‚Äî they‚Äôve got your back!"
        )

# =====================================================
# üîü FASTAPI ROUTES
# =====================================================
@app.get("/check")
async def check():
    return {"status": "ok"}

@app.get("/", response_class=HTMLResponse)
async def get_chat(request: Request):
    global response_count
    response_count = 0
    return templates.TemplateResponse("chat.html", {"request": request})

@app.post("/chat")
async def chat_endpoint(request: Request):
    data = await request.json()
    user_input = data.get("message", "")
    full_response = get_full_response(user_input)
    log_to_google_sheet(user_input, full_response)
    return JSONResponse({"response": full_response})

@app.get("/widget", response_class=HTMLResponse)
async def get_widget(request: Request):
    global response_count
    response_count = 0
    return templates.TemplateResponse("rukbot-widget.html", {"request": request})

@app.post("/refresh-knowledge")
async def refresh_knowledge():
    """
    Pull latest Drive files, rebuild chunks+embeddings in-memory.
    """
    global knowledge_files, knowledge_index
    try:
        print("üîÑ Refreshing RukBot Knowledge Base...")
        knowledge_files = load_google_folder_files(GOOGLE_DRIVE_FOLDER_ID)
        knowledge_index = build_knowledge_index(knowledge_files)
        print("‚úÖ Knowledge base refreshed successfully.")
        return JSONResponse(
            {"status": "success", "message": "Knowledge base refreshed successfully."}
        )
    except Exception as e:
        print(f"‚ùå Error refreshing knowledge base: {e}")
        return JSONResponse({"status": "error", "message": str(e)}, status_code=500)
